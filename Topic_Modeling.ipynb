{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T22:15:43.070445Z",
     "start_time": "2017-12-11T22:15:43.058875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Topic Modeling on Track Maven Social Media Data\n",
    "#\n",
    "# Ben Shaver\n",
    "# December 2017\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T18:39:41.682691Z",
     "start_time": "2017-12-11T18:39:41.673167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Topic modeling is a form of unsupervised learning. Like other unsupervised learning techniques, it learns patterns in a set of unlabelled \n",
    "# data, or data without a target variable to be predicted. In this case, the patterns learned are latent 'topics' that appear in a set of \n",
    "# texts, or documents. \n",
    "# Latent Dirichlet Allocation, or LDA, is a form of topic modeling that assumes documents are just bags of words, ignoring syntax and grammar.\n",
    "# LDA assumes documents are a mix of topics. Each word in a document belongs to each topic with a fixed probabiltiy unique to that document, and\n",
    "# each topic in turn returns a given word with a set of unique probabilities unique to that topic.\n",
    "# The purpose of LDA is to approximate the assumed 'latent' distribution which represents the mix of topics across documents AND the mix of\n",
    "# words across topics. Once the LDA model is trained, it can be used to compute the mix of topics for a particular document, and to compute\n",
    "# the mix of words per topic. Note that each word is not unique to a topic, but merely more likely to appear for a given topic.\n",
    "# \n",
    "# Below, I wrap the Python package Gensim's functionality into a class and some helper functions in order to train a model on social media\n",
    "# data from Track Maven. For each brand within the Conde Nast umbrella, LDA trains a model on the unique corpus of its facebook posts\n",
    "# (combining the different text fields first). Then a topic is assigned to each observation according to the topic most highly represnted\n",
    "# by that post. Finally, a CSV file is saved which is identical to the file read in except 'text' and 'topic' columns have been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T20:30:51.178345Z",
     "start_time": "2017-12-11T20:30:51.124707Z"
    }
   },
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    '''A class that takes a list or pandas Series of strings as input and outputs a trained LDA model'''\n",
    "    # Credit to Matt Brems' LDA lecture for the LDA basics\n",
    "    \n",
    "    def __init__(self, num_topics=5, passes=20):\n",
    "        # Number of topics to find\n",
    "        self.num_topics = num_topics\n",
    "        \n",
    "        # Number of passes over the data to make. More passes will ensure the convergence on the 'correct' \n",
    "        #  latent distribution of topics across documents and words across topics.\n",
    "        self.passes = passes\n",
    "        \n",
    "        # Initialize the tokenizer object\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        # Fetch an English stop words list from the NLTK package\n",
    "        self.en_stop = get_stop_words('en')\n",
    "\n",
    "        # Initialize a 'stemmer' object which will reduce words to 'stems'\n",
    "        self.stemmer = PorterStemmer()    \n",
    "\n",
    "    def transform(self, text_series):\n",
    "        '''Transforms a series of texts into a dictionary and a corpus, both saved as attributes of the object'''\n",
    "        self.text_series = text_series\n",
    "        \n",
    "        # Initialize empty list to contain tokenized strings\n",
    "        tokenized_text = []\n",
    "        \n",
    "        # Loop through text_series\n",
    "        for text in text_series:\n",
    "\n",
    "            # Turn each string into a series of lowercase words\n",
    "            raw = text.lower()\n",
    "            tokens = self.tokenizer.tokenize(raw)\n",
    "\n",
    "            # Remove stop words\n",
    "            tokens = [text for text in tokens if not text in self.en_stop]\n",
    "\n",
    "            # Turn words into 'stems,' to reduce the total number of unique words\n",
    "            tokens = [self.stemmer.stem(text) for text in tokens]\n",
    "\n",
    "            # Remove strings shorter than 4 elements\n",
    "            tokens = [text for text in tokens if len(text) > 3]\n",
    "\n",
    "            # Add tokens to list\n",
    "            tokenized_text.append(tokens)\n",
    "\n",
    "        # Create a id:term dictionary from our tokenized series of strings\n",
    "        self.dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "        # Create a document-term matrix from our tokenized series of strings\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in tokenized_text]   \n",
    "        \n",
    "     \n",
    "    def train_model(self):\n",
    "        '''Train the model. Uses Gensims multiple core implementation of the LDA model.''' \n",
    "        self.model = gensim.models.ldamulticore.LdaMulticore(self.corpus, num_topics=self.num_topics, id2word = self.dictionary, passes=self.passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T20:36:41.581040Z",
     "start_time": "2017-12-11T20:36:41.561489Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_topic_string(topic, n_words=5, join=True):\n",
    "    '''Return a list of words charcterizing each topic'''\n",
    "    topic_words = [lda.model.show_topic(topic)[i][0] for i in range(n_words)]\n",
    "    if join:\n",
    "        topic_words = ' '.join(topic_words)\n",
    "    return(topic_words)\n",
    "\n",
    "def fetch_doc_topic(document, n_words=5, num_topics=5):\n",
    "    '''Return the topic most represented by a text. Minimum string length (for error handling) is 5.'''\n",
    "    if type(document) != str:\n",
    "        return('')\n",
    "    if len(document) < 5:\n",
    "        return('')\n",
    "    probs = lda.model[lda.dictionary.doc2bow(document.split())]\n",
    "    probs = [probs[i][1] for i in range(len(probs))]\n",
    "    topic = np.argmax(probs)\n",
    "    return(fetch_topic_string(topic, n_words=n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T18:39:49.565362Z",
     "start_time": "2017-12-11T18:39:41.812035Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benps\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data. Be careful with encoding! There are strange characters.\n",
    "facebook = pd.read_csv('assets/facebook_data.csv', encoding='ISO-8859-1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T18:39:58.280667Z",
     "start_time": "2017-12-11T18:39:49.799992Z"
    }
   },
   "outputs": [],
   "source": [
    "def safe_string_add(*args):\n",
    "    '''Safely adds multiple strings, ignores non-string inputs.'''\n",
    "    string = ''\n",
    "    for arg in args:\n",
    "        if type(arg) == str:\n",
    "            string += ' ' + arg\n",
    "    return(string)        \n",
    "    \n",
    "# Create a column of all the text from each FB post\n",
    "facebook['Text'] = [safe_string_add(facebook['media_title'][i],\n",
    "         facebook['message'][i]) for i in range(facebook.shape[0])]\n",
    "\n",
    "# If you'd like to do a similar analysis for Instagram data, \n",
    "# simply import the IG data and combine all text fields in a 'Text' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T18:39:58.593997Z",
     "start_time": "2017-12-11T18:39:58.573944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the unique brands represented\n",
    "brands = facebook['brand_name'].unique()\n",
    "\n",
    "# Replace 'facebook' with 'intsagram' here and below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T18:39:58.949440Z",
     "start_time": "2017-12-11T18:39:58.873238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate new dataframe to store data.\n",
    "facebook_topics = pd.DataFrame(columns=list(facebook.columns) + ['Topic'])\n",
    "\n",
    "# For each brand, train an LDA model and assign each observation to one of 5 topics. Append to pre-existing dataframe.\n",
    "# This will take a while.\n",
    "\n",
    "# for brand in brands:\n",
    "#     try:\n",
    "#         brand_data = facebook[facebook['brand_name'] == brand]\n",
    "\n",
    "#         lda = LDA(num_topics=5, passes=20)\n",
    "#         lda.transform(brand_data['Text'])\n",
    "#         lda.train_model()\n",
    "#         print(brand + ' analyzed.')\n",
    "#         brand_data['Topic'] = [fetch_doc_topic(text, num_topics=5) for text in brand_data['Text']]\n",
    "#         facebook_topics = facebook_topics.append(brand_data)\n",
    "#         facebook_topics.to_csv('assets/fb_w_topics.csv')\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T21:45:47.949477Z",
     "start_time": "2017-12-11T21:37:16.127271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teen_Vogue analyzed.\n"
     ]
    }
   ],
   "source": [
    "# Teen Vogue has the most FB posts, by a significant margin. Let's focus on Teen Vogue, and try and determine how to\n",
    "# develop distinct topics (not too many, not too few) and a sensible name for a topic.\n",
    "brand = 'Teen_Vogue'\n",
    "brand_data = facebook[facebook['brand_name'] == brand]\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "lda = LDA(num_topics=num_topics, passes=30)\n",
    "lda.transform(brand_data['Text'])\n",
    "lda.train_model()\n",
    "\n",
    "print(brand + ' analyzed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T21:46:10.624420Z",
     "start_time": "2017-12-11T21:45:48.424332Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_doc_topics(document, n_words=5, num_topics=5):\n",
    "    '''Return the topic most represented by a text. Minimum string length (for error handling) is 5.'''\n",
    "    if type(document) != str:\n",
    "        return([1/num_topics]*num_topics)\n",
    "        # If the document is not a string, there is a uniform likelihood across all topics\n",
    "    if len(document) < 5:\n",
    "        return([1/num_topics]*num_topics)\n",
    "        # If the document is fewer than 5 characters, lets also say it could be from any topic\n",
    "    probs = lda.model[lda.dictionary.doc2bow(document.split())]\n",
    "    # Returns num_topics (topic,probability) tuples\n",
    "    probs = [item[1] for item in probs] # Extract just the probabilities\n",
    "\n",
    "    return(probs)\n",
    "\n",
    "# Construct data frame of probabilities corresponding to each topic:\n",
    "foo = pd.DataFrame([fetch_doc_topics(doc) for doc in brand_data['Text']]) \n",
    "# Add a row identifying the most likely category for each.\n",
    "foo['Main_topic'] = foo.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T21:46:11.183559Z",
     "start_time": "2017-12-11T21:46:11.129294Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_data.reset_index(inplace=True) # Reset index of original data frame and:\n",
    "brand_data = pd.concat([brand_data, foo], axis=1) # Concatenate together column-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T21:46:11.648708Z",
     "start_time": "2017-12-11T21:46:11.612109Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_text = [brand_data.loc[brand_data['Main_topic'] == i,'Text'].str.cat(sep = ';') for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T21:46:12.083052Z",
     "start_time": "2017-12-11T21:46:12.074840Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T21:46:12.571329Z",
     "start_time": "2017-12-11T21:46:12.560636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize TfIdf vectorizer, with encoding to handle strange values.\n",
    "# We're goint to look at the most 'distinctive' 2 or 3 word phrases across each of the raw strings with all the text from each topic\n",
    "# So, for example, we can discover that the two word phrase 'Selena Gomez' is very distinctive and unique to only one of the topics.\n",
    "tf = TfidfVectorizer(encoding='ISO-8859-1', ngram_range=(2,4), max_features = num_topics*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T23:05:13.175105Z",
     "start_time": "2017-12-11T23:04:58.834840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns the most 'distinctive' 2-, 3-, or 4- word phrase for each topic, and sets as column name\n",
    "topic_names = [tf.get_feature_names()[np.argmax(i)] for i in tf.fit_transform(raw_text).todense()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T23:23:34.837661Z",
     "start_time": "2017-12-11T23:23:34.786524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign generated topic names.\n",
    "\n",
    "topic_dict = dict({(i,topic_names[i]) for i in range(num_topics)})\n",
    "\n",
    "brand_data = brand_data.rename(columns = topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T23:27:11.828403Z",
     "start_time": "2017-12-11T23:27:06.251824Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_data.to_csv('assets/TeenVogueFB_wTopics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T23:24:34.979128Z",
     "start_time": "2017-12-11T23:24:34.972110Z"
    }
   },
   "outputs": [],
   "source": [
    "#### To be carried on in separate 'modeling' script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T22:44:28.785108Z",
     "start_time": "2017-12-11T22:44:28.778088Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-11T23:20:45.180339Z",
     "start_time": "2017-12-11T23:20:45.162571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3    0.098330\n",
       "0.7    0.447361\n",
       "Name: impact, dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_data['impact'].quantile([.3,.7])\n",
    "\n",
    "# Build target variable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
