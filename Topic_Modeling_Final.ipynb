{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T14:11:28.841865Z",
     "start_time": "2017-12-12T14:11:15.407738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benps\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling on Track Maven Social Media Data\n",
    "#\n",
    "# Ben Shaver\n",
    "# December 2017\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.style.use(['fivethirtyeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T14:11:28.871376Z",
     "start_time": "2017-12-12T14:11:28.860729Z"
    }
   },
   "outputs": [],
   "source": [
    "# Topic modeling is a form of unsupervised learning. Like other unsupervised learning techniques, it learns patterns in a set of unlabelled \n",
    "# data, or data without a target variable to be predicted. In this case, the patterns learned are latent 'topics' that appear in a set of \n",
    "# texts, or documents. \n",
    "# Latent Dirichlet Allocation, or LDA, is a form of topic modeling that assumes documents are just bags of words, ignoring syntax and grammar.\n",
    "# LDA assumes documents are a mix of topics. Each word in a document belongs to each topic with a fixed probabiltiy unique to that document, and\n",
    "# each topic in turn returns a given word with a set of unique probabilities unique to that topic.\n",
    "# The purpose of LDA is to approximate the assumed 'latent' distribution which represents the mix of topics across documents AND the mix of\n",
    "# words across topics. Once the LDA model is trained, it can be used to compute the mix of topics for a particular document, and to compute\n",
    "# the mix of words per topic. Note that each word is not unique to a topic, but merely more likely to appear for a given topic.\n",
    "# \n",
    "# Below, I wrap the Python package Gensim's functionality into a class and some helper functions in order to train a model on social media\n",
    "# data from Track Maven. For each brand within the Conde Nast umbrella, LDA trains a model on the unique corpus of its facebook posts\n",
    "# (combining the different text fields first). Then a topic is assigned to each observation according to the topic most highly represnted\n",
    "# by that post. Finally, a CSV file is saved which is identical to the file read in except 'text' and 'topic' columns have been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T14:11:28.948523Z",
     "start_time": "2017-12-12T14:11:28.889106Z"
    }
   },
   "outputs": [],
   "source": [
    "class LDA:\n",
    "    '''A class that takes a list or pandas Series of strings as input and outputs a trained LDA model'''\n",
    "    # Credit to Matt Brems' LDA lecture for the LDA basics\n",
    "    \n",
    "    def __init__(self, num_topics=5, passes=20):\n",
    "        # Number of topics to find\n",
    "        self.num_topics = num_topics\n",
    "        \n",
    "        # Number of passes over the data to make. More passes will ensure the convergence on the 'correct' \n",
    "        #  latent distribution of topics across documents and words across topics.\n",
    "        self.passes = passes\n",
    "        \n",
    "        # Initialize the tokenizer object\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        # Fetch an English stop words list from the NLTK package\n",
    "        self.en_stop = get_stop_words('en')\n",
    "\n",
    "        # Initialize a 'stemmer' object which will reduce words to 'stems'\n",
    "        self.stemmer = PorterStemmer()    \n",
    "\n",
    "    def transform(self, text_series):\n",
    "        '''Transforms a series of texts into a dictionary and a corpus, both saved as attributes of the object'''\n",
    "        self.text_series = text_series\n",
    "        \n",
    "        # Initialize empty list to contain tokenized strings\n",
    "        tokenized_text = []\n",
    "        \n",
    "        # Loop through text_series\n",
    "        for text in text_series:\n",
    "\n",
    "            # Turn each string into a series of lowercase words\n",
    "            raw = text.lower()\n",
    "            tokens = self.tokenizer.tokenize(raw)\n",
    "\n",
    "            # Remove stop words\n",
    "            tokens = [text for text in tokens if not text in self.en_stop]\n",
    "\n",
    "            # Turn words into 'stems,' to reduce the total number of unique words\n",
    "            tokens = [self.stemmer.stem(text) for text in tokens]\n",
    "\n",
    "            # Remove strings shorter than 4 elements\n",
    "            tokens = [text for text in tokens if len(text) > 3]\n",
    "\n",
    "            # Add tokens to list\n",
    "            tokenized_text.append(tokens)\n",
    "\n",
    "        # Save texts for later\n",
    "        self.texts = tokenized_text\n",
    "            \n",
    "        # Create a id:term dictionary from our tokenized series of strings\n",
    "        self.dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "        # Create a document-term matrix from our tokenized series of strings\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in tokenized_text]   \n",
    "        \n",
    "     \n",
    "    def train_model(self):\n",
    "        '''Train the model. Uses Gensims multiple core implementation of the LDA model.''' \n",
    "        self.model = gensim.models.ldamulticore.LdaMulticore(self.corpus, num_topics=self.num_topics, id2word = self.dictionary, passes=self.passes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T14:11:28.990607Z",
     "start_time": "2017-12-12T14:11:28.969017Z"
    }
   },
   "outputs": [],
   "source": [
    "def fetch_topic_string(topic, n_words=5, join=True):\n",
    "    '''Return a list of words charcterizing each topic'''\n",
    "    topic_words = [lda.model.show_topic(topic)[i][0] for i in range(n_words)]\n",
    "    if join:\n",
    "        topic_words = ' '.join(topic_words)\n",
    "    return(topic_words)\n",
    "\n",
    "def fetch_doc_topics(document, num_topics=5):\n",
    "    '''Return the topic most represented by a text. Minimum string length (for error handling) is 5.'''\n",
    "    if type(document) != str:\n",
    "        return([1/num_topics]*num_topics)\n",
    "        # If the document is not a string, there is a uniform likelihood across all topics\n",
    "    if len(document) < 5:\n",
    "        return([1/num_topics]*num_topics)\n",
    "        # If the document is fewer than 5 characters, lets also say it could be from any topic\n",
    "    probs = lda.model[lda.dictionary.doc2bow(document.split())]\n",
    "    # Returns num_topics (topic,probability) tuples\n",
    "    probs = [item[1] for item in probs] # Extract just the probabilities\n",
    "\n",
    "    return(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T14:11:37.960402Z",
     "start_time": "2017-12-12T14:11:29.007652Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benps\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in the data. Be careful with encoding! There are strange characters.\n",
    "facebook = pd.read_csv('assets/facebook_data.csv', encoding='ISO-8859-1', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T14:11:47.324320Z",
     "start_time": "2017-12-12T14:11:38.169460Z"
    }
   },
   "outputs": [],
   "source": [
    "def safe_string_add(*args):\n",
    "    '''Safely adds multiple strings, ignores non-string inputs.'''\n",
    "    string = ''\n",
    "    for arg in args:\n",
    "        if type(arg) == str:\n",
    "            string += ' ' + arg\n",
    "    return(string)        \n",
    "    \n",
    "# Create a column of all the text from each FB post\n",
    "facebook['Text'] = [safe_string_add(facebook['media_title'][i],\n",
    "         facebook['message'][i]) for i in range(facebook.shape[0])]\n",
    "\n",
    "# If you'd like to do a similar analysis for Instagram data, \n",
    "# simply import the IG data and combine all text fields in a 'Text' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:05:49.416739Z",
     "start_time": "2017-12-12T16:05:49.396224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the unique brands represented\n",
    "brands = facebook['brand_name'].unique()\n",
    "\n",
    "# Replace 'facebook' with 'intsagram' here and below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:05:49.891524Z",
     "start_time": "2017-12-12T16:05:49.879492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate new dataframe to store data.\n",
    "facebook_topics = pd.DataFrame(columns=list(facebook.columns) + ['Topic'])\n",
    "\n",
    "# For each brand, train an LDA model and assign each observation to one of 5 topics. Append to pre-existing dataframe.\n",
    "# This will take a while.\n",
    "\n",
    "# for brand in brands:\n",
    "#     try:\n",
    "#         brand_data = facebook[facebook['brand_name'] == brand]\n",
    "\n",
    "#         lda = LDA(num_topics=5, passes=20)\n",
    "#         lda.transform(brand_data['Text'])\n",
    "#         lda.train_model()\n",
    "#         print(brand + ' analyzed.')\n",
    "#         brand_data['Topic'] = [fetch_doc_topic(text, num_topics=5) for text in brand_data['Text']]\n",
    "#         facebook_topics = facebook_topics.append(brand_data)\n",
    "#         facebook_topics.to_csv('assets/fb_w_topics.csv')\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:05:54.756963Z",
     "start_time": "2017-12-12T16:05:54.678756Z"
    }
   },
   "outputs": [],
   "source": [
    "# Teen Vogue has the most FB posts, by a significant margin. Let's focus on Teen Vogue, and try and determine how to\n",
    "# develop distinct topics (not too many, not too few) and a sensible name for a topic.\n",
    "\n",
    "# May take about 20 minutes with 20 passes. \n",
    "\n",
    "# Using 3 topics based on topic coherence metric. See below.\n",
    "\n",
    "brand = 'Teen_Vogue'\n",
    "brand_data = facebook[facebook['brand_name'] == brand]\n",
    "\n",
    "num_topics = 3\n",
    "\n",
    "lda = LDA(num_topics=num_topics, passes=20)\n",
    "lda.transform(brand_data['Text'])\n",
    "\n",
    "lda.train_model()\n",
    "\n",
    "print(brand + ' analyzed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:29.824231Z",
     "start_time": "2017-12-12T16:06:01.771663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct data frame of probabilities corresponding to each topic:\n",
    "foo = pd.DataFrame([fetch_doc_topics(doc) for doc in brand_data['Text']]) \n",
    "# Add a row identifying the most likely category for each.\n",
    "foo['Main_topic'] = foo.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:30.341675Z",
     "start_time": "2017-12-12T16:06:30.288210Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_data.reset_index(inplace=True) # Reset index of original data frame and:\n",
    "brand_data = pd.concat([brand_data, foo], axis=1) # Concatenate together column-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:30.726654Z",
     "start_time": "2017-12-12T16:06:30.716104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now we have a dataframe representing the original Teen Vogue data, plus additional columns for each topic\n",
    "# that LDA found, and a column representing the 'main topic' for each post, we want to provide some label to the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:31.198921Z",
     "start_time": "2017-12-12T16:06:31.162313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by main topics and concatenate all text into a list of raw text for each topics.\n",
    "raw_text = [brand_data.loc[brand_data['Main_topic'] == i,'Text'].str.cat(sep = ';') for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:34.205955Z",
     "start_time": "2017-12-12T16:06:34.197934Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:35.092315Z",
     "start_time": "2017-12-12T16:06:34.924872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize TfIdf vectorizer, with encoding to handle strange values.\n",
    "# We're goint to look at the most 'distinctive' 2 or 3 word phrases across each of the raw strings with all the text from each topic\n",
    "# So, for example, we can discover that the two word phrase 'Selena Gomez' is very distinctive and unique to only one of the topics.\n",
    "tf = TfidfVectorizer(encoding='ISO-8859-1', ngram_range=(2,4), max_features = num_topics*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:49.997567Z",
     "start_time": "2017-12-12T16:06:35.502568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns the most 'distinctive' 2-, 3-, or 4- word phrase for each topic, and sets as column name\n",
    "topic_names = [tf.get_feature_names()[np.argmax(i)] for i in tf.fit_transform(raw_text).todense()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:50.557302Z",
     "start_time": "2017-12-12T16:06:50.463570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign generated topic names (if they're any good)\n",
    "\n",
    "topic_dict = dict({(i,topic_names[i]) for i in range(num_topics)})\n",
    "\n",
    "topic_dict = dict({0:'Topic 1', 1:'Topic 2', 2:'Topic 3'})\n",
    "\n",
    "brand_data = brand_data.rename(columns = topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:50.930283Z",
     "start_time": "2017-12-12T16:06:50.901205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['taylor swift trump justin bieber just year school peopl will',\n",
       " 'jenner hadid kendal look gigi just kyli selena gomez fashion',\n",
       " 'girl just know instagram beauti need thing will best love']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are these 2-4 word ngrams good topic names? Maybe we would be better off going with the first option, just \n",
    "# using the top outputed words per topic:\n",
    "[fetch_topic_string(i, n_words=10) for i in range(num_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:56.847488Z",
     "start_time": "2017-12-12T16:06:51.363435Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_data.to_csv('assets/'+brand+'_FB_w3Topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:57.521118Z",
     "start_time": "2017-12-12T16:06:57.509783Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate through a range of possible values for number of topics, so we can identify which maximizes topic coherence.\n",
    "\n",
    "# from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# coherence_scores = []\n",
    "# for i in range(2,10):\n",
    "#     brand = 'Teen_Vogue'\n",
    "#     brand_data = facebook[facebook['brand_name'] == brand]\n",
    "\n",
    "#     num_topics = i\n",
    "\n",
    "#     lda = LDA(num_topics=num_topics, passes=20)\n",
    "#     lda.transform(brand_data['Text'])\n",
    "\n",
    "#     lda.train_model()\n",
    "#     cm = CoherenceModel(model=lda.model, texts=lda.texts, dictionary=lda.dictionary, coherence='u_mass')\n",
    "#     coherence_scores.append(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:06:59.840872Z",
     "start_time": "2017-12-12T16:06:58.125609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.12172968831\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(model=lda.model, texts=lda.texts, dictionary=lda.dictionary, coherence='u_mass')\n",
    "print(cm.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:07:00.299126Z",
     "start_time": "2017-12-12T16:07:00.292607Z"
    }
   },
   "outputs": [],
   "source": [
    "# How do we model with LDA results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:07:34.012368Z",
     "start_time": "2017-12-12T16:07:33.974266Z"
    }
   },
   "outputs": [],
   "source": [
    "brand_data.drop([3,4], axis=1, inplace=True) # Since we only ended up using 3 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:15:48.205217Z",
     "start_time": "2017-12-12T16:15:48.182655Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.435002</td>\n",
       "      <td>0.057862</td>\n",
       "      <td>0.507136</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666474</td>\n",
       "      <td>0.166758</td>\n",
       "      <td>0.166768</td>\n",
       "      <td>0.098939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.445626</td>\n",
       "      <td>0.443240</td>\n",
       "      <td>0.111134</td>\n",
       "      <td>0.038389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.171792</td>\n",
       "      <td>0.180911</td>\n",
       "      <td>0.647296</td>\n",
       "      <td>10.236497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.901071</td>\n",
       "      <td>0.048610</td>\n",
       "      <td>0.050319</td>\n",
       "      <td>0.068004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.192044</td>\n",
       "      <td>0.255549</td>\n",
       "      <td>0.552407</td>\n",
       "      <td>0.043913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.227028</td>\n",
       "      <td>0.208510</td>\n",
       "      <td>0.564462</td>\n",
       "      <td>0.072138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.117703</td>\n",
       "      <td>0.768663</td>\n",
       "      <td>0.113634</td>\n",
       "      <td>0.108336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.058009</td>\n",
       "      <td>0.279908</td>\n",
       "      <td>0.662083</td>\n",
       "      <td>0.048282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.754082</td>\n",
       "      <td>0.126551</td>\n",
       "      <td>0.119367</td>\n",
       "      <td>0.007540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic 1   Topic 2   Topic 3     impact\n",
       "2   0.435002  0.057862  0.507136   0.008185\n",
       "5   0.666474  0.166758  0.166768   0.098939\n",
       "6   0.445626  0.443240  0.111134   0.038389\n",
       "7   0.171792  0.180911  0.647296  10.236497\n",
       "9   0.901071  0.048610  0.050319   0.068004\n",
       "11  0.192044  0.255549  0.552407   0.043913\n",
       "12  0.227028  0.208510  0.564462   0.072138\n",
       "13  0.117703  0.768663  0.113634   0.108336\n",
       "14  0.058009  0.279908  0.662083   0.048282\n",
       "16  0.754082  0.126551  0.119367   0.007540"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T15:54:33.262905Z",
     "start_time": "2017-12-12T15:54:33.254239Z"
    }
   },
   "outputs": [],
   "source": [
    "# As a regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T16:27:49.108238Z",
     "start_time": "2017-12-12T16:27:49.096832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'brand', 'content', 'engagement', 'has_spend', 'id', 'impact',\n",
       "       'share_token', 'timestamp', 'type', 'urls', 'channel_type', 'channel',\n",
       "       'brand_name', 'comments', 'content_type', 'like_count', 'media_caption',\n",
       "       'media_title', 'message', 'permalink', 'picture_url', 'angry_count',\n",
       "       'haha_count', 'love_count', 'sad_count', 'wow_count', 'shares',\n",
       "       'reaction_count', 'Text', 'Topic 1', 'Topic 2', 'Topic 3',\n",
       "       'Main_topic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T15:54:33.784291Z",
     "start_time": "2017-12-12T15:54:33.774688Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:17.360368Z",
     "start_time": "2017-12-12T17:15:17.001853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benps\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model_data = brand_data[['Topic 1','Topic 2','Topic 3','impact']]\n",
    "model_data.dropna(inplace=True)\n",
    "model_data = model_data[model_data['Topic 1'] != model_data['Topic 2']]\n",
    "X = model_data[['Topic 1','Topic 2','Topic 3']]\n",
    "y = model_data['impact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:18.290377Z",
     "start_time": "2017-12-12T17:15:18.264774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17054182, -0.00779063,  0.17833246])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:18.834319Z",
     "start_time": "2017-12-12T17:15:18.819276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009796922020294252"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a very good R-squared. Unfortunately just 3 topics by themselves can't predict impact as a linear process.\n",
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:19.308794Z",
     "start_time": "2017-12-12T17:15:19.294756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32625638,  0.29778658,  0.0284698 ])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take the log of the impact values and predict that instead.\n",
    "y = np.log(y)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:19.817647Z",
     "start_time": "2017-12-12T17:15:19.805615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0073529744777988126"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our R-squared is an order of magnitude better, but still bad.\n",
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:20.263915Z",
     "start_time": "2017-12-12T17:15:20.256315Z"
    }
   },
   "outputs": [],
   "source": [
    "# As a classification task:\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:20.824516Z",
     "start_time": "2017-12-12T17:15:20.716110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build target variable here. We will just predict if post is above or below the median level of impact.\n",
    "discrete_y = [1 if x > y.median() else 0 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:21.402469Z",
     "start_time": "2017-12-12T17:15:21.302702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53394803017602688"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not a great accuracy. But a direction of influence can still be inferred, below:\n",
    "\n",
    "logreg = LogisticRegressionCV()\n",
    "logreg.fit(X, discrete_y)\n",
    "logreg.score(X, discrete_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-12T17:15:21.930398Z",
     "start_time": "2017-12-12T17:15:21.920370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04100391,  0.04438467, -0.00338224]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
